# Day16 (3.1)

### 1.过拟合

- 定义：模型在训练集上表现很好，但在测试数据集上表现很差的现象，失去实用价值
- 原因：
  - 模型过于复杂，学习到了训练数据中的噪声和随机波动
  - 训练数据过少或数据代表性不强
  - 训练时间过长，死记硬背样本
- 解决办法：
  - 降低模型复杂度(减少网络层数，降低多项式层数)
  - 数据增强
  - L1，L2正则化
  - 早停

### 2.数据归一化

- 定义：将不同尺度，不同量纲的特征数据缩放到一个统一范围，如0到1或-1到1，使得特征对模型影响更加均衡
- 常用办法：![image-20260228144530603](https://winterhoildayaixly.oss-cn-shenzhen.aliyuncs.com/image-20260228144530603.png)
- 作用：
  - 加速梯度下降收敛，提高训练效率
  - 避免数值不稳定
  - 让不同量级的特征在模型中有同等重要性

### 3.正则化

- 定义：在损失函数中加入惩罚项，限制模型参数的大小，防止模型过于复杂，缓解过拟合
- 常用办法：![image-20260228145105208](https://winterhoildayaixly.oss-cn-shenzhen.aliyuncs.com/image-20260228145105208.png)
- 核心思想：牺牲些许训练，换取更强泛化能力

### 4.数据清洗：

- 定义：对原始数据进行预处理，去除噪声，纠正错误，填补缺失值等
- 核心步骤：
  - 缺失值处理：删除含缺失值的特征，或者用相应数据类型的中位数，众数，均值填充
  - 重复值处理：检测并删除重复样本
  - 异常值检测：使用箱线图等方法识别处理异常值/极端值
  - 数据类型转换：将文本，类别等数据转换为可处理的数值类型(如独热编码，标签编码)
  - 噪声过滤：平滑处理或去除明显随机噪声
- 意义：高质量的数据是训练出好模型的原因

### 5.BatchSize与学习率

- BatchSIze
  - 定义：每次训练时，一次性输入模型的样本数量
  - 影响：
    - 小BS：训练速度快，内存占用小，梯度估计噪声大，收敛路径更曲折，有利于跳出局部最优
    - 大BS：梯度更加稳定，收敛更平滑，但内存占用大，容易陷入局部最优，泛化能力可能变差
  - 经验值：需要根据硬件和任务调整，常见取值为32，64，128，256
- 学习率
  - 定义：控制模型参数更新步长的超参数，决定了每次梯度下降时参数调整的幅度
  - 影响：
    - 学习率过小：收敛速度慢，训练时间长，容易陷入局部最优
    - 学习率过大：参数更新步长太大，可能导致模型在最优解附件震荡，甚至无法收敛
  - 调优策略：学习率衰减或自适应学习率算法

