# Day15(2.28)

### 1.前向传播

- 前向传播本质：神经网络进行预测和推理的过程

- 激活函数：

  - 激活函数是神经网络的“心脏”，它为网络注入了非线性，使其能够学习复杂的模式

  - | 激活函数                     | 公式                                                         | 特点与应用场景                                               |
    | ---------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
    | Sigmoid                      | ![image-20260221162520032](https://winterhoildayaixly.oss-cn-shenzhen.aliyuncs.com/image-20260221162520032.png) | 将输出压缩在 (0,1) 之间，常用于二分类输出层；易出现梯度消失问题。 |
    | ReLU (Rectified Linear Unit) | ![image-20260221162530568](https://winterhoildayaixly.oss-cn-shenzhen.aliyuncs.com/image-20260221162530568.png) | 计算简单，缓解梯度消失，是隐藏层的首选；存在 “死神经元” 问题。 |
    | Tanh                         | ![image-20260221162539246](https://winterhoildayaixly.oss-cn-shenzhen.aliyuncs.com/image-20260221162539246.png) | 将输出压缩在 (-1,1) 之间，比 Sigmoid 更常用，但仍有梯度消失风险。 |
    | Leaky ReLU                   | ![image-20260221162549294](https://winterhoildayaixly.oss-cn-shenzhen.aliyuncs.com/image-20260221162549294.png) | 解决了 ReLU 的 “死神经元” 问题，为负输入提供一个小的梯度。   |

- 单个神经元的向前传播：
  - 一个神经元的计算过程可以分成三步
  - 过程：
    - 1.线性组合：将输入与对应权重相乘后求和，再加上偏置z=w1x1+w2x2+⋯+wnxn+b=w⋅x+b
    - 2.激活变换：将线性组合的结果z输入到激活函数f中，a = f（z）
    - 3.输出结果：激活后的结果a作为该神经元的输出，传递给下一个神经元
- 由神经元组成的网络向前传播：
  - 整个网络向前传播是所有神经元计算的有序集合
  - 过程：
    - 1.输入层：接受原始数据x
    - 2.隐藏层1：![image-20260221163205407](https://winterhoildayaixly.oss-cn-shenzhen.aliyuncs.com/image-20260221163205407.png)
    - 3.隐藏层2（如果有）：![image-20260221163234078](https://winterhoildayaixly.oss-cn-shenzhen.aliyuncs.com/image-20260221163234078.png)
    - 4.输出层：![image-20260221163407756](https://winterhoildayaixly.oss-cn-shenzhen.aliyuncs.com/image-20260221163407756.png)

​                                        （其中y^就是网络的最终预测值）

### 2.反向传播

- 反向传播本质：神经网络进行学习和训练的核心算法，能够最小化预测误差

- 损失函数：

  - 损失函数是神经网络预测值和真实值之间差距的量化指标

  - | 任务类型   | 常用损失函数           | 公式                                                         | 适用场景                         |
    | :--------- | ---------------------- | ------------------------------------------------------------ | -------------------------------- |
    | 回归任务   | 均方误差（MSE）        | ![image-20260221163959333](https://winterhoildayaixly.oss-cn-shenzhen.aliyuncs.com/image-20260221163959333.png) | 预测连续值，如房价、温度。       |
    | 二分类任务 | 交叉熵 (Cross-Entropy) | ![image-20260221164034814](https://winterhoildayaixly.oss-cn-shenzhen.aliyuncs.com/image-20260221164034814.png) | 判断 “是 / 否”，如垃圾邮件分类。 |
    | 多分类任务 | 多类交叉熵             | ![image-20260221164114413](https://winterhoildayaixly.oss-cn-shenzhen.aliyuncs.com/image-20260221164114413.png) | 区分多个类别，如手写数字识别。   |

- 过程：
  - 反向传播是一个迭代优化的过程，主要包括以下步骤
  - 1.前向传播的基础：计算预测值和损失
  - 2.输出层梯度：计算损失对输出层权重 W(L) 和偏置 b(L) 的梯度。![image-20260221164440035](https://winterhoildayaixly.oss-cn-shenzhen.aliyuncs.com/image-20260221164440035.png)
  - 3.反向传递梯度：利用链式法则，将梯度逐层传递到前面的隐藏层，计算每一层的 δ(l)![image-20260221164517419](https://winterhoildayaixly.oss-cn-shenzhen.aliyuncs.com/image-20260221164517419.png)
  - 4.更新权重：使用梯度下降等优化器，根据计算出的梯度更新所有参数![image-20260221164602737](https://winterhoildayaixly.oss-cn-shenzhen.aliyuncs.com/image-20260221164602737.png)

​      	（其中 η 是学习率，控制更新的步长）

### 3.监督学习，无监督学习，半监督学习

- 监督学习：
  - 定义：从有标签的数据中学习，训练数据既有输入特征，又有对应的真实标签（如图片加猫的组合）
  - 目标：学习一个从输入到输出的映射函数，对新的，未见过的数据进行预测
  - 典型应用：分类(识别图片中的物体),回归(预测股票价格)
- 无监督学习：
  - 定义：从无标签的数据中发现内在结构，训练数据只有输入特征，没有对应的标签
  - 目标：发现数据中的聚类，降维，关联规则等
  - 典型任务：聚类(将用户分成不同群体)，降维(PCA),异常检测
- 半监督学习：
  - 定义：结合少量有标签数据和大量无标签数据进行学习
  - 目标：利用无标签数据的分布信息，提升在少量有标签数据上的学习效果，解决标签数据稀缺的问题
  - 典型应用：在医疗影像分析、自然语言处理等领域，获取高质量标签成本高昂时非常有效